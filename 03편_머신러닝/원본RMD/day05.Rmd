---
title: "fastcampus_머신러닝_5"
author: "huimin"
date: '2019 7 16 '
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# 기초 설정
```{r}
library(tidyverse)
library(readr)
library(caret)
library(e1071)
library(MLmetrics)
```


# 의사결정나무 알고리즘의 개요


**의사결정나무는 분류모형에 사용되는 대표적인 알고리즘**이다. 전체 데이터셋을 몇 개의 소그룹으로 분류하는데, 각각의 소그룹에는 목표변수의 레벨이 하나로 집중된다.


전체 데이터셋에서 **소그룹으로 분리되는 규칙**을 따라가다 보면 데이터셋이 가지는 일정한 패턴을 규칙으로 만들고, 이는 이해하기 쉽고 활용하기 좋아 많은 분석가들이 분류모형을 적합할 때 의사결정나무 알고리즘을 사용한다.




의사결정나무의 **특징**은 다음과 같다.


- 목표변수가 **범주형일 때 분류모형, 연속형일 때 회귀모형을 적합할 수 있다.**
- **순수도(Purity) 또는 불순도(Impurity)를 기준**으로 나무를 성장시킨다.
- 부모마디보다 자식마디의 순수도가 높아지거나 불순도가 낮아져야 분리될 수 있다.
- 나무모형을 적합하고 나면 **IF-THEN 규칙**을 만들 수 있으므로, 해석하기가 쉽다.




의사결정나무의 **장점**은 다음과 같다.


- **비모수적 알고리즘(데이터셋에 대한 통계적 가정이 필요없다.)**이기 때문에 간단하다.
- 이상치는 분류과정에서 자연스럽게 도태되기 때문에 **이상치에 덜 민감하다.**
- 결측값이 있어도 모형을 적합할 수 있다.




의사결정나무의 **단점**은 다음과 같다.


- 과적합하기 쉽다.
- 정지규칙을 별도로 설정하여, 데이터에 맞도록 나무의 성장을 제한해야 한다.
- **비용복잡도를 기준으로 가지치기를 통하여 과적합의 위험을 줄일 수 있다.**


# 의사결정나무 알고리즘의 종류


**CART(Classification and Regression Tree)**


- 분류모형은 **지니지수를 이용하여 순수도를 계산**한다.
- 회귀모형은 분산의 감소량을 기준으로 나무를 성장시킨다.
- **이지분리(Binary Classification)**에 사용된다.
- 반복 분할을 하므로 같은 입력변수가 여러 번 사용된다.
- R에서 rpart 패키지의 **rpart() 함수**를 사용한다.


**C5.0**


- **엔트로피**를 이용하여 **순수도를 계산**하는 알고리즘이다.
- 부모마디의 엔트로피에서 자식마디의 엔트로피 가중평균을 뺀 감소량인 **정보이익(Information Gain)**을 기준으로 나무를 성장시킨다.
- 다지분리가 가능하다.
- **반복 분할을 하므로, 같은 입력변수를 여러 번 사용한다.**
- R에서는 C50 패키지의 **C5.0() 함수**를 사용한다.


**CHAID(Chi-squared Automatic Interaction Detection)**


- 분류모형은 **카이제곱 통계량을 이용하여 순수도를 계산**한다.
- 회귀모형의 경우 F-통계량을 이용한다.
- **입력변수를 반드시 범주형**이어야 한다. 숫자형 벡터를 전부 범주형으로 변환해야한다.
- CHAID 패키지의 **chaid() 함수**를 사용한다.


# 의사결정나무 프로세스


![Caption](img/day05_1.jpg)


- 뿌리마디로부터 출발하여, 부모마디보다 자식마디의 **순수도가 높아지거나 불순도가 낮아지는 최적의 분리규칙이 있을 때 나무를 성장**시킨다.
- **정지규칙을 만족하면 성장**을 중단한다.
- 과적합을 피해 불필요한 가지를 잘라내는 **가지치기를 하여 나무를 완성**한다.
- 다수의 분류모형을 적합했다면, **다양한 성능지표를 통해 최종모형을 결정**한다.


# 자세한 의사결정나무 프로세스별 설명


![Caption](img/day05_2.jpg)


![Caption](img/day05_3.jpg)


![Caption](img/day05_4.jpg)


![Caption](img/day05_5.jpg)


![Caption](img/day05_6.jpg)


![Caption](img/day05_7.jpg)


![Caption](img/day05_8.jpg)


![Caption](img/day05_9.jpg)


![Caption](img/day05_10.jpg)


# 의사결정나무 함수 설명


![Caption](img/day05_11.jpg)


![Caption](img/day05_12.jpg)


![Caption](img/day05_13.jpg)


# 이지분리 의사결정나무 실습


## 시작 : 데이터 전처리 과정
```{r}
# 실습데이터 불러오기
load(file = "practice_data/bank.RData")

# 데이터 구조 파악하기
str(bank)

# 간단한 데이터 실습을 위해서 ID와 ZIP CODE 삭제하기
bank <- bank[,c(-1,-5)]

# 요약통계량을 통해 대략적인 모양 확인하기
summary(bank)

# 목표변수를 범주형으로 변환하기
bank$PersonalLoan <- as.factor(bank$PersonalLoan)

# 목표변수 비율 확인하기
bank$PersonalLoan %>% table() %>% prop.table()

# 매우 불균형한 데이터이다. 하지만 우선, 훈련셋, 시험셋으로 나누기로 한다.
set.seed(123)

index <- sample(x = 1:2,
                size = nrow(bank),
                prob = c(0.7, 0.3),
                replace = TRUE)

trainset <- bank[index == 1, ]
testset <- bank[index == 2, ]

trainset$PersonalLoan %>% table() %>% prop.table()
testset$PersonalLoan %>% table() %>% prop.table()
```


## 1. 이지분리 의사결정나무 모형 적합하기
```{r}
# 의사결정나무 모형 적합에 필요한 패키지이다.
library(rpart)

fit.tree0 <- rpart(formula = PersonalLoan ~.,
                   data = trainset,
                   parms = list(split = "gini"),
                   control = rpart.control(minsplit = 20,
                                           cp = 0.01,
                                           maxdepth = 10))

summary(fit.tree0)
```


출력되는 결과는 많지만, **두 번째 표와 세 번째 표가 가장 중요하다.**


**두 번째 표**는 비용복잡도(CP) 파라미터별로 가지가 분리되는 횟수, 실제 오차 및 교차검정 오차를 제시한다. 비용복잡도 파라미터를 기본값인 0.01로 정했기 때문에 그 기준으로 분리된 횟수는 4번이다. 따라서 끝마디 수는 5개가 된다.


**세 번째 표**는 변수별 중요도를 보여준다. Income, Education, Family, Mortagage, CDAccount, Experience 순으로 오분류율이 낮은 모형을 적합하는데 기여하였다.


**네 번째부터는** 노드 별로 결과를 상세하게 나타내어 준다. 간단하게 Node number 1을 살펴보자면, 비용복잡도 약 0.27 추정라벨 0, 예상 손실은 약 0.09이다. 그리고 왼쪽 자식마디로 2755건이 할당되고, 오른쪽 자식마디로는 758건이 할당되었다. 분리규칙 중에서 Improve가 가장 큰 분리규칙이 사용되었다. 이는 즉, Income이 135.97보다 크면 오른쪽 자식마디로 이동하고, 그보다 작을경우 왼쪽 자식마디로 이동한다는 뜻이다.


## 2. rpart.plot 패키지를 이용하여 나무모형 그림 그려보기
```{r}
library(rpart.plot)

rpart.plot(fit.tree0)
```


이 외의 기본 함수를 이용한 나무모형 그림이 있지만, 생략하겠다.


## 3. 의사결정나무 가지치기
```{r}
# 가지치기 판단을 하기 위해서 비용복잡도 표를 출력한다.
printcp(fit.tree0)

# 현재의 모형은 가지치기가 필요가 없다. 하지만, 실습을 위해 조금 더 복잡한 새로운 모형을 만들고 가지치기를 해보도록 하겠다.
fit.tree1 <- rpart(formula = PersonalLoan ~.,
                   data = trainset,
                   parms = list(split = "gini"),
                   control = rpart.control(minsplit = 20,
                                           cp = 0.001,
                                           maxdepth = 30))

# 새롭게 적합한 모형의 비용복잡도 표 출력
printcp(fit.tree1)

# xerror가 최소일 때의 지표들을 best.xerror 객체에 저장한다.
best.xerror <- fit.tree1$cptable[fit.tree1$cptable[, "xerror"] == min(fit.tree1$cptable[, 4]), ]

best.xerror
names(best.xerror)

# best.cp 객체에 cp값 저장하기
best.cp <- best.xerror[1]


# prune.rpart 함수를 통해서 가지치기하기
fit.tree2 <- prune.rpart(tree = fit.tree1, cp = best.cp)

# 가지치기한 모형 확인하기
summary(fit.tree2)
rpart.plot(fit.tree2)
```


## 4. 가지치기 전과 후의 모형 평가하기
```{r}
pred <- predict(fit.tree0, newdata = trainset, type = "class")
pred.prun <- predict(fit.tree2, data = trainset, type = "class")
real <- trainset$PersonalLoan

confusionMatrix(pred, real, positive = "1")
confusionMatrix(pred.prun, real, positive = "1")

F1_Score(real, pred)
F1_Score(real, pred.prun)
```


가지치기 후의 모형이 조금 더 높은 성능을 보임을 알 수 있다.